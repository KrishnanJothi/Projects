{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Selection of cross_encoder model (line.58)\n",
    "2. \"n\" value (line.147)\n",
    "3. \"k\" value (line.150)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import models, losses, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import csv\n",
    "import sys\n",
    "import tqdm\n",
    "import math\n",
    "import gzip\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "\n",
    "def get_random_n_pairs(List, n):\n",
    "    pairs = []\n",
    "    for _ in range(n):\n",
    "       pair = random.sample(List, 2)\n",
    "       pair[0] = str(pair[0])\n",
    "       pair[1] = str(pair[1])\n",
    "       pairs.append(pair)\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def select_top_k(silver_data, silver_scores,k):\n",
    "    top_k = [x + [y] for x, y in zip(silver_data, silver_scores)]\n",
    "    top_k = sorted(top_k, key=lambda x: x[2], reverse=True)\n",
    "    top_k = top_k[:k]\n",
    "    removed = [l.pop(2) for l in top_k]\n",
    "    return top_k, numpy.array(removed)\n",
    "\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "# supressing INFO messages for elastic-search logger\n",
    "tracer = logging.getLogger('elasticsearch')\n",
    "tracer.setLevel(logging.CRITICAL)\n",
    "es = Elasticsearch()\n",
    "\n",
    "#You can specify any huggingface/transformers pre-trained model here, for example, bert-base-uncased, roberta-base, xlm-roberta-base\n",
    "cross_encoder_model_name = 'bert-base-uncased'\n",
    "bi_encoder_model_name = 'sentence-transformers/paraphrase-distilroberta-base-v1'\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "max_seq_length = 128\n",
    "\n",
    "###### Read Datasets ######\n",
    "\n",
    "#Check if dataset exsist. If not, download and extract  it\n",
    "Gold_dataset_path = 'gold_dataset.tsv'\n",
    "\n",
    "#Cross and bi encoder path\n",
    "cross_encoder_path = 'model/cross-encoder/'+cross_encoder_model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "bi_encoder_path = 'model/bi-encoder/'+bi_encoder_model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "###### Cross-encoder (simpletransformers) ######\n",
    "logging.info(\"Loading sentence-transformers model: {}\".format(cross_encoder_model_name))\n",
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for cross-encoder model\n",
    "cross_encoder = CrossEncoder(cross_encoder_model_name, num_labels=1)\n",
    "\n",
    "\n",
    "###### Bi-encoder (sentence-transformers) ######\n",
    "logging.info(\"Loading bi-encoder model: {}\".format(bi_encoder_model_name))\n",
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "word_embedding_model = models.Transformer(bi_encoder_model_name, max_seq_length=max_seq_length)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "bi_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# Step 1: Train cross-encoder model with Gold dataset\n",
    "#\n",
    "#####################################################################\n",
    "\n",
    "logging.info(\"Step 1: Train cross-encoder: ({}) with Gold dataset\".format(cross_encoder_model_name))\n",
    "\n",
    "gold_samples = []\n",
    "dev_samples = []\n",
    "test_samples = []\n",
    "\n",
    "with open(Gold_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t')#, quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        score = float(row['score'])   # Normalize score to range 0 ... 1\n",
    "\n",
    "        if row['split'] == 'dev':\n",
    "            dev_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "        elif row['split'] == 'test':\n",
    "            test_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "        else:\n",
    "            #As we want to get symmetric scores, i.e. CrossEncoder(A,B) = CrossEncoder(B,A), we pass both combinations to the train set\n",
    "            gold_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "            gold_samples.append(InputExample(texts=[row['sentence2'], row['sentence1']], label=score))\n",
    "\n",
    "\n",
    "# We wrap gold_samples (which is a List[InputExample]) into a pytorch DataLoader\n",
    "train_dataloader = DataLoader(gold_samples, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# We add an evaluator, which evaluates the performance during training\n",
    "evaluator = CECorrelationEvaluator.from_input_examples(dev_samples, name='Gold-dev_split')\n",
    "\n",
    "# Configure the training\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "# Train the cross-encoder model\n",
    "cross_encoder.fit(train_dataloader=train_dataloader,\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=cross_encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#\n",
    "# Step 2: Label sampled unlabeled dataset(silver dataset) using cross-encoder model\n",
    "#\n",
    "############################################################################\n",
    "\n",
    "# Reading CSV of unlabeled dataset\n",
    "col_list = ['definitions']\n",
    "df = pd.read_csv('all_entries.csv', sep=',',\n",
    "                 usecols=col_list).replace('\"', '', regex=True)\n",
    "\n",
    "# \"n\" no.of sentence pairs to be generated from unlabeled dataset\n",
    "n = 50000\n",
    "\n",
    " # Selecting top \"k\" pairs based on their similarity scores\n",
    "k = 1000\n",
    "\n",
    "silver_data = get_random_n_pairs(df['definitions'].values.tolist(), n)\n",
    "\n",
    "logging.info(\"Number of generated pairs from unlabeled dataset: {}\".format(len(silver_data)))\n",
    "logging.info(\"Step 2.2: Label the generated pairs with cross-encoder: {}\".format(cross_encoder_model_name))\n",
    "\n",
    "cross_encoder = CrossEncoder(cross_encoder_path)\n",
    "silver_scores = cross_encoder.predict(silver_data)\n",
    "\n",
    "#logging.info(\"Number of silver pairs selected from the labeled pairs: {}\".format(k))\n",
    "#silver_data, silver_scores = select_top_k(silver_data, silver_scores, k)\n",
    "\n",
    "# All model predictions should be between [0,1]\n",
    "assert all(0.0 <= score <= 1.0 for score in silver_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "#\n",
    "# Step 3: Train bi-encoder model with both gold + silver dataset - Augmented SBERT\n",
    "#\n",
    "#################################################################################################\n",
    "\n",
    "logging.info(\"Step 3: Train bi-encoder: {} with gold + silver dataset\".format(bi_encoder_model_name))\n",
    "\n",
    "# Convert the dataset to a DataLoader ready for training\n",
    "logging.info(\"Read gold and silver train dataset\")\n",
    "silver_samples = list(InputExample(texts=[data[0], data[1]], label=score) for data, score in zip(silver_data, silver_scores))\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(gold_samples + silver_samples, shuffle=True, batch_size=batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=bi_encoder)\n",
    "\n",
    "logging.info(\"Read development dataset\")\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
    "\n",
    "# Configure the training.\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs  * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "# Train the bi-encoder model\n",
    "bi_encoder.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=bi_encoder_path\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "#\n",
    "# Evaluate Augmented SBERT performance on STS benchmark (test) dataset\n",
    "#\n",
    "######################################################################\n",
    "\n",
    "# load the stored augmented-sbert model\n",
    "bi_encoder = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v1')\n",
    "logging.info(\"Read test dataset\")\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='Gold-test_split')\n",
    "test_evaluator(bi_encoder, output_path=bi_encoder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
