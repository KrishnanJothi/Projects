{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "    \n",
    "source_dir = 'datasets_labeled'  \n",
    "file_names = os.listdir(source_dir)\n",
    "file_names = [file for file in file_names if file.endswith('.xlsx')]\n",
    "\n",
    "for ind, filename in enumerate(file_names):\n",
    "    target_dir = \"datasets_labeled/removed/missing_\"+ os.path.splitext(filename)[0] +\"/\"\n",
    "    os.makedirs(target_dir)\n",
    "    for index, file_name in enumerate(file_names):\n",
    "        if index != ind:\n",
    "            shutil.copy(os.path.join(source_dir, file_name), target_dir)\n",
    " \n",
    "n = 0\n",
    "for ind, filename in enumerate(file_names):\n",
    "    target_dir = \"datasets/removed/missing_\"+ os.path.splitext(filename)[0] +\"/\"\n",
    "    os.makedirs(target_dir)\n",
    "    n = n+1\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "rootdir = 'datasets_labeled/removed'\n",
    "\n",
    "for [subdir, dirs, files], [subdir1, dirs1, files1] in zip(os.walk(rootdir), os.walk('datasets/removed')):\n",
    "    print(subdir, str(subdir1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pairs_from_file(directory, filename):\n",
    "    term = filename.split('.')[0]\n",
    "    df_rawgroups = read_excel(directory + filename, sheet_name=\"groups\", index_col=None, header=None, engine='openpyxl')\n",
    "    df_rawgroups = df_rawgroups.drop(df_rawgroups.columns[[0, 1]], axis=1)  # removing first and third unwanted column\n",
    "    df_rawgroups = df_rawgroups.drop([0, 1])  # removing first and third unwanted column\n",
    "\n",
    "    df_group_similarities = read_excel(directory + filename, sheet_name=\"group similarities\", index_col=None, header=None, engine='openpyxl')\n",
    "    df_group_similarities = df_group_similarities.drop(0, axis=0)\n",
    "    df_group_similarities = df_group_similarities.drop(0, axis=1)\n",
    "\n",
    "    df_definition = df_rawgroups[df_rawgroups.columns[range(0, df_rawgroups.shape[1]-1, 4)]]\n",
    "\n",
    "    groups = []\n",
    "    for column in df_definition.columns:\n",
    "        group = []\n",
    "        for definition in df_definition[column]: \n",
    "            if not pd.isna(definition): group.append(definition)\n",
    "        groups.append(group)    \n",
    "        pairs = []\n",
    "\n",
    "    for grp_index1, group1 in enumerate(groups):\n",
    "        for definition1 in group1:\n",
    "            for grp_index2, group2 in enumerate(groups):\n",
    "                for definition2 in group2:\n",
    "                    if definition1 is not definition2:\n",
    "                        sim = df_group_similarities.iloc[grp_index1,grp_index2]\n",
    "                        if type(sim) is str: sim = sim.upper() \n",
    "                        if len(str(sim)) == 1 and (grp_index1 <= grp_index2) and not pd.isna(sim): \n",
    "                            pairs.append([term, term + str(grp_index1), term + str(grp_index2), definition1, definition2, sim])\n",
    "    return pairs\n",
    "\n",
    "def generationProcess(dataset_size, directory,loc_generated_devset,loc_generated_trainset,loc_generated_testset):\n",
    "    pairs = []                  \n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"~\") and filename.endswith(\".xlsx\"):\n",
    "            print(os.path.join(directory, filename))\n",
    "            pairs.extend(get_all_pairs_from_file(directory, filename))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    df_pairs = pd.DataFrame(pairs, columns=[\"term\", \"grp_id1\", \"grp_id2\", \"definition1\", \"definition2\", \"sim\"])\n",
    "    \n",
    "    print(90*'=')\n",
    "    print(\"Datenset zusammenstellen: Möglichst ausgeglichene Anzahl von similarity labels,\\nBenennungen und Begriffsgruppen erstellen\")\n",
    "    print(90*'=' + \"\\n\")\n",
    "\n",
    "    df = df_pairs.copy(deep=True)\n",
    "    df.insert(df.shape[1], \"count_sim\", [0]*df.shape[0], True) \n",
    "    df.insert(df.shape[1], \"count_term\", [0]*df.shape[0], True) \n",
    "    df.insert(df.shape[1], \"count_grpid1\", [0]*df.shape[0], True) \n",
    "    df.insert(df.shape[1], \"count_grpid2\", [0]*df.shape[0], True) \n",
    "    df.insert(df.shape[1], \"count_def\", [0]*df.shape[0], True) \n",
    "\n",
    "    df_dataset = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    for i in range (0, dataset_size):\n",
    "        #erstes Paar auswählen und entfernen\n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "        select = df.iloc[0]\n",
    "        df_dataset = df_dataset.append(select)\n",
    "        df.drop(select.name, inplace=True, errors='raise')\n",
    "\n",
    "        #eigenschaften hochzählen\n",
    "        df.loc[df.sim == select.sim, 'count_sim'] += 1\n",
    "        df.loc[df.term == select.term, 'count_term'] += 1\n",
    "        df.loc[df.grp_id1 == select.grp_id1, 'count_grpid1'] += 1\n",
    "        df.loc[df.grp_id1 == select.grp_id2, 'count_grpid2'] += 1\n",
    "        df.loc[df.grp_id2 == select.grp_id1, 'count_grpid1'] += 1\n",
    "        df.loc[df.grp_id2 == select.grp_id2, 'count_grpid2'] += 1\n",
    "        df.loc[df.definition1 == select.definition1, 'count_def'] += 1\n",
    "        df.loc[df.definition1 == select.definition2, 'count_def'] += 1\n",
    "        df.loc[df.definition2 == select.definition1, 'count_def'] += 1\n",
    "        df.loc[df.definition2 == select.definition2, 'count_def'] += 1\n",
    "\n",
    "        #sortieren, so dass Paare mit Eigenschaften, die am seltensten vertreten sind, oben stehen \n",
    "        df.sort_values(by=['count_sim','count_def','count_grpid1','count_grpid2','count_term'], inplace=True)\n",
    "\n",
    "\n",
    "    #Verteilung ausgeben\n",
    "    print(df_dataset.groupby(['sim']).size())\n",
    "    print('')\n",
    "    print(df_dataset.groupby(['term']).size())\n",
    "    \n",
    "    print(90*'=')\n",
    "    print(\"Datenset speichern\")\n",
    "    print(90*'=' + \"\\n\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    testset_terms = [\"domain\", \"hazard\", \"group\", \"event\"]\n",
    "\n",
    "    train_df = df_dataset[~df_dataset[\"term\"].isin(testset_terms)]\n",
    "    test_df = df_dataset[df_dataset[\"term\"].isin(testset_terms)]\n",
    "    test_df.to_csv(loc_generated_testset)\n",
    "\n",
    "\n",
    "    train, dev = train_test_split(train_df, test_size=0.2)\n",
    "    train.to_csv(loc_generated_trainset)\n",
    "    dev.to_csv(loc_generated_devset)\n",
    "\n",
    "    #for term in df_dataset[\"term\"].unique():\n",
    "    #    df_dataset[df_dataset[\"term\"] == term].to_csv(\"datasets_by_term/\"+term+\".csv\")\n",
    "        \n",
    "    #test_df\n",
    "    \n",
    "    #train_df[train_df[\"term\"]==\"client\"]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from itertools import product\n",
    "from pandas import read_excel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Number of pairs\n",
    "dataset_size = 7000\n",
    "\n",
    "rootdir = 'datasets_labeled/removed/'\n",
    "generated_dataset_dir = 'datasets/removed/'\n",
    "count = 0\n",
    "\n",
    "for subdir1, dirs1, files1 in os.walk(generated_dataset_dir):\n",
    "    if subdir1 != generated_dataset_dir and not subdir1.endswith(\".ipynb_checkpoints\"):\n",
    "        directory = rootdir+subdir1[17:]+\"/\"\n",
    "\n",
    "        print(90*'=')\n",
    "        print(\"Generating dataset in directory: \" + subdir1 + \"--> \"+str(count+1)+\"/\"+str(n))\n",
    "        print(90*'=' + \"\\n\")\n",
    "    \n",
    "        loc_generated_devset = subdir1 + \"/dev_set.csv\"\n",
    "        loc_generated_trainset = subdir1 + \"/train_set.csv\"\n",
    "        loc_generated_testset = subdir1 + \"/test_set.csv\"\n",
    "        \n",
    "        generationProcess(dataset_size, directory,loc_generated_devset,loc_generated_trainset,loc_generated_testset)\n",
    "        \n",
    "        count=count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pandas import read_csv\n",
    "from itertools import product\n",
    "from pandas import read_excel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def gen_dataframe(dataset_dir, filename, split):\n",
    "    file = dataset_dir + filename\n",
    "    col_list = [\"sim\", \"definition1\", \"definition2\"]\n",
    "    dataframe = read_csv(file, usecols=col_list)\n",
    "    dataframe[\"split\"] = split\n",
    "    #dataframe.loc[dataframe[\"sim\"] == \"1\", \"sim\"] = [1.0]\n",
    "    #dataframe.loc[dataframe[\"sim\"] == \"A\", \"sim\"] = [0.8]\n",
    "    #dataframe.loc[dataframe[\"sim\"] == \"B\", \"sim\"] = [0.6]\n",
    "    #dataframe.loc[dataframe[\"sim\"] == \"C\", \"sim\"] = [0.4]\n",
    "    #dataframe.loc[dataframe[\"sim\"] == \"D\", \"sim\"] = [0.0]\n",
    "    dataframe.columns = ['sentence1', 'sentence2', 'score', 'split']\n",
    "    \n",
    "    dataframe[\"score\"] = [str(i).replace(\",\", \"\") for i in dataframe[\"score\"]]\n",
    "    \n",
    "    #dataframe['score']=dataframe['score'].to_string()\n",
    "    \n",
    "   \n",
    "    return dataframe\n",
    "\n",
    "dataset_dir = 'datasets/removed/'\n",
    "\n",
    "for subdir1, dirs1, files1 in os.walk(dataset_dir):\n",
    "    if subdir1 != dataset_dir and not subdir1.endswith(\".ipynb_checkpoints\"):\n",
    "        directory = dataset_dir+subdir1[17:]+\"/\"\n",
    "  \n",
    "        \n",
    "        frames = [gen_dataframe(directory,\"train_set.csv\", \"train\"), gen_dataframe(directory,\"test_set.csv\", \"test\"), gen_dataframe(directory,\"dev_set.csv\", \"dev\")]\n",
    "        combined = pd.concat(frames)\n",
    "        combined[\"index\"] = range(0, 0+len(combined))\n",
    "        combined.set_index(\"index\", inplace=True)\n",
    "        combined.to_csv(directory + '/' + \"gold_dataset.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "def evaluate(dataset_path, trained_model='fine-tuned_model/fine-tuning-distilroberta-base-paraphrase-v1-2021-05-10_14-17-46'):\n",
    "    train_samples = []\n",
    "    dev_samples = []\n",
    "    test_samples = []\n",
    "\n",
    "    with open(dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            if row['score'] == \"1\":\n",
    "                score = 1\n",
    "            elif row['score'] == \"A\":\n",
    "                score = 0.8\n",
    "            elif row['score'] == \"B\":\n",
    "                score = 0.6\n",
    "            elif row['score'] == \"C\":\n",
    "                score = 0.4\n",
    "            else: score = 0\n",
    "    \n",
    "\n",
    "            inp_example = InputExample(texts=[str(row['sentence1']), str(row['sentence2'])], label=score)\n",
    "\n",
    "            test_samples.append(inp_example)\n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "    #\n",
    "    # Load the model and evaluate its performance on test dataset\n",
    "    #\n",
    "    ##############################################################################\n",
    "    model = SentenceTransformer('distilroberta-base-paraphrase-v1')\n",
    "    #model = SentenceTransformer(trained_model)\n",
    "    test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='test')\n",
    "    test_evaluator(model, output_path=trained_model)\n",
    "\n",
    "\n",
    "dataset_dir = 'datasets/removed/'\n",
    "\n",
    "for subdir1, dirs1, files1 in os.walk(dataset_dir):\n",
    "    if subdir1 != dataset_dir and not subdir1.endswith(\".ipynb_checkpoints\"):\n",
    "        directory = dataset_dir+subdir1[17:]+\"/\"+\"gold_dataset.tsv\"\n",
    "        \n",
    "        print(subdir1[17:])\n",
    "        evaluate(directory)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import pandas\n",
    "\n",
    "file = \"plots/With_missing_terms_new_on_base_model.csv\"\n",
    "col_list = [\"cosine_spearman\"]\n",
    "dataframe = read_csv(file, usecols=col_list)\n",
    "\n",
    "\n",
    "x_axis = []\n",
    "dataset_dir = 'datasets/removed/'\n",
    "for subdir1, dirs1, files1 in os.walk(dataset_dir):\n",
    "    if subdir1 != dataset_dir and not subdir1.endswith(\".ipynb_checkpoints\"):\n",
    "        directory = dataset_dir+subdir1[17:]+\"/\"+\"gold_dataset.tsv\"  \n",
    "        x_axis.append(str(subdir1[17:]))\n",
    "\n",
    "        \n",
    "df=pandas.concat([dataframe, pd.DataFrame(x_axis, columns=['on_base_model'])],axis=1)\n",
    "df.to_csv(\"plots/plot2_values_new.csv\")\n",
    "fig=df.plot.bar(x='on_base_model', y='cosine_spearman',figsize=(12, 8), rot=90).get_figure()\n",
    "fig.savefig(\"plots/plot2_new_on_base_model.jpg\",bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "def evaluate(dataset_path, threshold, trained_model='fine-tuned_model/fine-tuning-distilroberta-base-paraphrase-v1-2021-05-10_14-17-46'):\n",
    "    train_samples = []\n",
    "    dev_samples = []\n",
    "    test_samples = []\n",
    "\n",
    "    with open(dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            if row['score'] == \"1\":\n",
    "                score = 1\n",
    "            elif row['score'] == \"A\":\n",
    "                score = 0.8\n",
    "            elif row['score'] == \"B\":\n",
    "                score = 0.6\n",
    "            elif row['score'] == \"C\":\n",
    "                score = 0.4\n",
    "            else: score = 0\n",
    "                \n",
    "            if score >= threshold:\n",
    "                score = 1\n",
    "            else: score = 0\n",
    "\n",
    "            inp_example = InputExample(texts=[str(row['sentence1']), str(row['sentence2'])], label=score)\n",
    "\n",
    "            test_samples.append(inp_example)\n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "    #\n",
    "    # Load the model and evaluate its performance on test dataset\n",
    "    #\n",
    "    ##############################################################################\n",
    "    model = SentenceTransformer('distilroberta-base-paraphrase-v1')\n",
    "    test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='test')\n",
    "    test_evaluator(model, output_path=trained_model)\n",
    "\n",
    "\n",
    "dataset_dir = 'datasets/gold_dataset.tsv'\n",
    "\n",
    "for threshold in np.arange(0.2, 0.9, 0.05):\n",
    "    evaluate(dataset_dir, threshold)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import pandas\n",
    "\n",
    "file = \"plots/varied_threshold_on_base_model.csv\"\n",
    "col_list = [\"cosine_spearman\"]\n",
    "dataframe = read_csv(file, usecols=col_list)\n",
    "\n",
    "\n",
    "x_axis = []\n",
    "dataset_dir = 'datasets/removed/'\n",
    "for threshold in np.arange(0.2, 0.9, 0.05): \n",
    "    x_axis.append(threshold)\n",
    "\n",
    "        \n",
    "df=pandas.concat([dataframe, pd.DataFrame(x_axis, columns=['threshold (evaluation on base model)'])],axis=1)\n",
    "df.to_csv(\"plots/plot4_values.csv\")\n",
    "fig=df.plot(x='threshold (evaluation on base model)', y='cosine_spearman',figsize=(12, 8), rot=90).get_figure()\n",
    "fig.savefig(\"plots/plot4_on_base_model.jpg\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate(dataset_path, trained_model='fine-tuned_model/fine-tuning-distilroberta-base-paraphrase-v1-2021-05-10_14-17-46'):\n",
    "    train_samples = []\n",
    "    dev_samples = []\n",
    "    test_samples = []\n",
    "    \n",
    "    newdf = pd.DataFrame(columns=['sentence1','sentence2','split','score','finetuned_model_sim','base_model_sim'])\n",
    "    model = SentenceTransformer(trained_model)\n",
    "    model1 = SentenceTransformer('distilroberta-base-paraphrase-v1')\n",
    "    with open(dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        for row in reader:\n",
    "            if row['score'] == \"1\":\n",
    "                score = 1\n",
    "            elif row['score'] == \"A\":\n",
    "                score = 0.8\n",
    "            elif row['score'] == \"B\":\n",
    "                score = 0.6\n",
    "            elif row['score'] == \"C\":\n",
    "                score = 0.4\n",
    "            else: score = 0\n",
    "\n",
    "            #inp_example = InputExample(texts=[str(row['sentence1']), str(row['sentence2'])], label=score)\n",
    "\n",
    "            #test_evaluator = EmbeddingSimilarityEvaluator(sentences1=[str(row['sentence1'])], sentences2=[str(row['sentence2'])], scores=[score])\n",
    "            #test_evaluator(model, output_path=trained_model)\n",
    "            embed=model.encode([str(row['sentence1']), str(row['sentence2'])])\n",
    "            embed1=model1.encode([str(row['sentence1']), str(row['sentence2'])])\n",
    "            cosine_score = util.pytorch_cos_sim(embed[0], embed[1])\n",
    "            cosine_score1 = util.pytorch_cos_sim(embed1[0], embed1[1])\n",
    "            df = pd.DataFrame([[row['sentence1'], row['sentence2'], row['split'], score, float(cosine_score), float(cosine_score1)]], columns=['sentence1','sentence2','split','score','finetuned_model_sim','base_model_sim'])\n",
    "            newdf=pandas.concat([newdf, df], ignore_index=True)\n",
    "     \n",
    "    newdf.to_csv('datasets/gold_dataset1.tsv', index=False)\n",
    "\n",
    "\n",
    "dataset_dir = 'datasets/gold_dataset.tsv'\n",
    "\n",
    "evaluate(dataset_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_excel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Number of pairs\n",
    "dataset_size = 7000\n",
    "\n",
    "rootdir = 'datasets_labeled/removed/'\n",
    "generated_dataset_dir = 'datasets/removed/'\n",
    "\n",
    "for [subdir, dirs, files], [subdir1, dirs1, files1] in zip(os.walk(rootdir), os.walk(generated_dataset_dir)):\n",
    "    if subdir != rootdir and subdir1 != generated_dataset_dir and not subdir1.endswith(\".ipynb_checkpoints\"):\n",
    "        directory = subdir+\"/\"\n",
    "        print(subdir[0:25]+subdir1[17:])\n",
    "        \n",
    "        loc_generated_devset = generated_dataset_dir + \"/dev_set.csv\"\n",
    "        loc_generated_trainset = generated_dataset_dir + \"/train_set.csv\"\n",
    "        loc_generated_testset = generated_dataset_dir + \"/test_set.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
